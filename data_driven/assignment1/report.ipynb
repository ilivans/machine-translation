{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filling up results table\n",
    "columns = [\"samples, *1e3\", \"null symbol\", \"stemming\", \"prior\", \"log-likelihood, *1e4\", \"aer, *1e-2\"]\n",
    "results = [\n",
    "    [10, \"\", \"\", \"uniform\", -88, 56.8],\n",
    "    [10, \"\", \"\", \"ibm2\", -67, 55.9],\n",
    "    [10, \"\", \"\", \"beta\", -31, 47.8],\n",
    "    [10, \"\", \"eng (snowball)\", \"uniform\", -94, 58.2],\n",
    "    [10, \"\", \"eng (snowball)\", \"beta\", -38, 49.4],\n",
    "    [10, \"\", \"eng (threshold)\", \"beta\", -39, 48.9],\n",
    "    [10, \"\", \"cze (given)\", \"uniform\", -79, 49.3],\n",
    "    [10, \"\", \"cze (given)\", \"ibm2\", -59, 47.5],\n",
    "    [10, \"\", \"cze (given)\", \"beta\", -22, 39.7],\n",
    "    [100, \"\", \"cze (given)\", \"beta\", -288, 39.7],\n",
    "    [10, \"naive\", \"cze (given)\", \"beta\", -22, 39.6],\n",
    "]\n",
    "results = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all experiments models were trained for 10 iterations. And I've got the next results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples, *1e3</th>\n",
       "      <th>null symbol</th>\n",
       "      <th>stemming</th>\n",
       "      <th>prior</th>\n",
       "      <th>log-likelihood, *1e4</th>\n",
       "      <th>aer, *1e-2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>uniform</td>\n",
       "      <td>-88</td>\n",
       "      <td>56.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ibm2</td>\n",
       "      <td>-67</td>\n",
       "      <td>55.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>beta</td>\n",
       "      <td>-31</td>\n",
       "      <td>47.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>eng (snowball)</td>\n",
       "      <td>uniform</td>\n",
       "      <td>-94</td>\n",
       "      <td>58.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>eng (snowball)</td>\n",
       "      <td>beta</td>\n",
       "      <td>-38</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>eng (threshold)</td>\n",
       "      <td>beta</td>\n",
       "      <td>-39</td>\n",
       "      <td>48.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>cze (given)</td>\n",
       "      <td>uniform</td>\n",
       "      <td>-79</td>\n",
       "      <td>49.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>cze (given)</td>\n",
       "      <td>ibm2</td>\n",
       "      <td>-59</td>\n",
       "      <td>47.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>cze (given)</td>\n",
       "      <td>beta</td>\n",
       "      <td>-22</td>\n",
       "      <td>39.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100</td>\n",
       "      <td></td>\n",
       "      <td>cze (given)</td>\n",
       "      <td>beta</td>\n",
       "      <td>-288</td>\n",
       "      <td>39.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>naive</td>\n",
       "      <td>cze (given)</td>\n",
       "      <td>beta</td>\n",
       "      <td>-22</td>\n",
       "      <td>39.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    samples, *1e3 null symbol         stemming    prior  log-likelihood, *1e4  \\\n",
       "0              10                               uniform                   -88   \n",
       "1              10                                  ibm2                   -67   \n",
       "2              10                                  beta                   -31   \n",
       "3              10               eng (snowball)  uniform                   -94   \n",
       "4              10               eng (snowball)     beta                   -38   \n",
       "5              10              eng (threshold)     beta                   -39   \n",
       "6              10                  cze (given)  uniform                   -79   \n",
       "7              10                  cze (given)     ibm2                   -59   \n",
       "8              10                  cze (given)     beta                   -22   \n",
       "9             100                  cze (given)     beta                  -288   \n",
       "10             10       naive      cze (given)     beta                   -22   \n",
       "\n",
       "    aer, *1e-2  \n",
       "0         56.8  \n",
       "1         55.9  \n",
       "2         47.8  \n",
       "3         58.2  \n",
       "4         49.4  \n",
       "5         48.9  \n",
       "6         49.3  \n",
       "7         47.5  \n",
       "8         39.7  \n",
       "9         39.7  \n",
       "10        39.6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model with given Czech lemmatization and custom prior based on Beta distribution showed the best perfomance.\n",
    "\n",
    "It is worth mentioning that any of the tried english stemming strategies didn't given any positive results, which is quite unexpected.\n",
    "\n",
    "Also I've looked at several sentences alignments of the best model and found that specific alignments like 1987-1987, firms-firmy, Co.-Co were succefully determined. Thats why I didn't try to hardcode such knowledge.\n",
    "\n",
    "Adding null symbol into source sentences \"aer\" slightly improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code with branches for certain improvements can be found here: \n",
    "\n",
    "https://github.com/ilivans/machine-translation/tree/master/data_driven/assignment1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
